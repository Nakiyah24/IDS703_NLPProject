{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "      <th>date</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speaker_description</th>\n",
       "      <th>state_info</th>\n",
       "      <th>true_counts</th>\n",
       "      <th>mostly_true_counts</th>\n",
       "      <th>half_true_counts</th>\n",
       "      <th>mostly_false_counts</th>\n",
       "      <th>false_counts</th>\n",
       "      <th>pants_on_fire_counts</th>\n",
       "      <th>context</th>\n",
       "      <th>justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13847</td>\n",
       "      <td>5</td>\n",
       "      <td>90 percent of Americans \"support universal bac...</td>\n",
       "      <td>October 2, 2017</td>\n",
       "      <td>government regulation;polls and public opinion...</td>\n",
       "      <td>chris abele</td>\n",
       "      <td>Chris Abele is Milwaukee County Executive, a p...</td>\n",
       "      <td>wisconsin</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>a tweet</td>\n",
       "      <td>\"Universal\" is the term for background checks ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13411</td>\n",
       "      <td>1</td>\n",
       "      <td>Last year was one of the deadliest years ever ...</td>\n",
       "      <td>May 19, 2017</td>\n",
       "      <td>after the fact;congress;criminal justice;histo...</td>\n",
       "      <td>thom tillis</td>\n",
       "      <td>Thom Tillis is a Republican who serves as U.S....</td>\n",
       "      <td>north carolina</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>a press release supporting the Back The Blue A...</td>\n",
       "      <td>Sen. Thom Tillis, a North Carolina Republican,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10882</td>\n",
       "      <td>0</td>\n",
       "      <td>Bernie Sanders's plan is \"to raise your taxes ...</td>\n",
       "      <td>October 28, 2015</td>\n",
       "      <td>taxes</td>\n",
       "      <td>chris christie</td>\n",
       "      <td>Chris Christie announced June 6, 2023 that he ...</td>\n",
       "      <td>national</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>Boulder, Colo</td>\n",
       "      <td>Christie said that Sanders’s plan is \"to raise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20697</td>\n",
       "      <td>4</td>\n",
       "      <td>Voter ID is supported by an overwhelming major...</td>\n",
       "      <td>December 8, 2021</td>\n",
       "      <td>voter id laws</td>\n",
       "      <td>lee zeldin</td>\n",
       "      <td>Lee Zeldin is a Republican representing New Yo...</td>\n",
       "      <td>new york</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a Tweet</td>\n",
       "      <td>Zeldin claimed voter identification requiremen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6095</td>\n",
       "      <td>2</td>\n",
       "      <td>Says Barack Obama \"robbed Medicare (of) $716 b...</td>\n",
       "      <td>August 12, 2012</td>\n",
       "      <td>federal budget;history;medicare;retirement</td>\n",
       "      <td>mitt romney</td>\n",
       "      <td>Mitt Romney is a U.S. senator from Utah. He ra...</td>\n",
       "      <td>national</td>\n",
       "      <td>31</td>\n",
       "      <td>33</td>\n",
       "      <td>58</td>\n",
       "      <td>35</td>\n",
       "      <td>32</td>\n",
       "      <td>19</td>\n",
       "      <td>an interview on \"60 Minutes\"</td>\n",
       "      <td>Romney said, \"There's only one president that ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  label                                          statement  \\\n",
       "0  13847      5  90 percent of Americans \"support universal bac...   \n",
       "1  13411      1  Last year was one of the deadliest years ever ...   \n",
       "2  10882      0  Bernie Sanders's plan is \"to raise your taxes ...   \n",
       "3  20697      4  Voter ID is supported by an overwhelming major...   \n",
       "4   6095      2  Says Barack Obama \"robbed Medicare (of) $716 b...   \n",
       "\n",
       "               date                                            subject  \\\n",
       "0   October 2, 2017  government regulation;polls and public opinion...   \n",
       "1      May 19, 2017  after the fact;congress;criminal justice;histo...   \n",
       "2  October 28, 2015                                              taxes   \n",
       "3  December 8, 2021                                      voter id laws   \n",
       "4   August 12, 2012         federal budget;history;medicare;retirement   \n",
       "\n",
       "          speaker                                speaker_description  \\\n",
       "0     chris abele  Chris Abele is Milwaukee County Executive, a p...   \n",
       "1     thom tillis  Thom Tillis is a Republican who serves as U.S....   \n",
       "2  chris christie  Chris Christie announced June 6, 2023 that he ...   \n",
       "3      lee zeldin  Lee Zeldin is a Republican representing New Yo...   \n",
       "4     mitt romney  Mitt Romney is a U.S. senator from Utah. He ra...   \n",
       "\n",
       "       state_info  true_counts  mostly_true_counts  half_true_counts  \\\n",
       "0       wisconsin            1                   4                 5   \n",
       "1  north carolina            0                   2                 7   \n",
       "2        national           21                  20                27   \n",
       "3        new york            1                   2                 0   \n",
       "4        national           31                  33                58   \n",
       "\n",
       "   mostly_false_counts  false_counts  pants_on_fire_counts  \\\n",
       "0                    3             5                     2   \n",
       "1                    3             2                     0   \n",
       "2                   11            17                     8   \n",
       "3                    0             0                     0   \n",
       "4                   35            32                    19   \n",
       "\n",
       "                                             context  \\\n",
       "0                                            a tweet   \n",
       "1  a press release supporting the Back The Blue A...   \n",
       "2                                      Boulder, Colo   \n",
       "3                                            a Tweet   \n",
       "4                       an interview on \"60 Minutes\"   \n",
       "\n",
       "                                       justification  \n",
       "0  \"Universal\" is the term for background checks ...  \n",
       "1  Sen. Thom Tillis, a North Carolina Republican,...  \n",
       "2  Christie said that Sanders’s plan is \"to raise...  \n",
       "3  Zeldin claimed voter identification requiremen...  \n",
       "4  Romney said, \"There's only one president that ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6049\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"LIAR2/train.csv\")\n",
    "display(df.head())\n",
    "print(len(df[\"subject\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing to see length of unique subjects:\n",
      "\n",
      "    lst = []\n",
      "    df[\"subject\"] = df[\"subject\"].fillna(\"NONE\")\n",
      "    for object in df[\"subject\"]:\n",
      "        item = object.split(\";\")\n",
      "        lst.extend(item)\n",
      "\n",
      "    unique_items = set(lst)\n",
      "    print(len(lst))\n",
      "    print(len(unique_items))\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"testing to see length of unique subjects:\\n\n",
    "    lst = []\n",
    "    df[\"subject\"] = df[\"subject\"].fillna(\"NONE\")\n",
    "    for object in df[\"subject\"]:\n",
    "        item = object.split(\";\")\n",
    "        lst.extend(item)\n",
    "\n",
    "    unique_items = set(lst)\n",
    "    print(len(lst))\n",
    "    print(len(unique_items))\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'michigan': 0, 'terrorism': 1, 'polls and public opinion': 2, 'federal budget': 3, 'veterans': 4, 'wealth': 5, 'labor': 6, 'stimulus': 7, 'politifact en español': 8, 'corporations': 9, 'islam': 10, 'nuclear': 11, 'legal issues': 12, 'baseball': 13, 'facebook fact-checks': 14, 'ukraine': 15, 'weather': 16, 'after the fact': 17, 'unions': 18, 'city budget': 19, 'regulation': 20, 'ask politifact': 21, 'oregon': 22, 'poverty': 23, 'arizona': 24, 'corrections and updates': 25, 'obama birth certificate': 26, 'good enough to be true': 27, 'transparency': 28, 'deficit': 29, 'congress': 30, 'human rights': 31, 'illinois': 32, 'states': 33, 'kagan nomination': 34, 'ad watch': 35, 'campaign finance': 36, 'environment': 37, 'foreign policy': 38, 'jan. 6': 39, \"the 2018 california governor's race\": 40, 'retirement': 41, 'oil spill': 42, 'religion': 43, 'criminal justice': 44, 'medicaid': 45, 'city government': 46, '<UNK>': 47, 'county budget': 48, 'technology': 49, 'bankruptcy': 50, 'nevada': 51, 'message machine 2012': 52, 'welfare': 53, 'new jersey': 54, 'iraq': 55, 'supreme court': 56, 'bipartisanship': 57, 'taxes': 58, 'population': 59, 'autism': 60, 'elections': 61, 'tourism': 62, 'occupy wall street': 63, 'children': 64, 'pop culture': 65, 'census': 66, 'privacy issues': 67, 'texas': 68, 'medicare': 69, 'florida': 70, 'infrastructure': 71, 'transportation': 72, 'fake news': 73, 'lottery': 74, 'cap and trade': 75, 'county government': 76, '<NONE>': 77, 'florida amendments': 78, 'ethics': 79, 'punditfact': 80, 'message machine 2010': 81, 'alcohol': 82, 'housing': 83, 'this week - abc news': 84, 'homeless': 85, 'pensions': 86, 'new york': 87, 'consumer safety': 88, 'ohio': 89, 'lgbtq': 90, 'ebola': 91, 'debt': 92, 'katrina': 93, 'crime': 94, 'drugs': 95, 'government regulation': 96, 'nbc': 97, 'voting record': 98, 'space': 99, 'virginia': 100, 'voter id laws': 101, 'marijuana': 102, 'state budget': 103, 'candidate biography': 104, 'fires': 105, 'food safety': 106, 'health care': 107, 'global news service': 108, 'israel': 109, 'military': 110, 'gambling': 111, 'negative campaigning': 112, 'tampa bay 10 news': 113, 'animals': 114, 'workers': 115, 'sotomayor nomination': 116, 'natural disasters': 117, 'jobs': 118, 'death penalty': 119, 'message machine 2014': 120, 'health check': 121, 'gas prices': 122, 'coronavirus': 123, 'water': 124, 'immigration': 125, 'new hampshire 2012': 126, 'women': 127, 'education': 128, 'party support': 129, 'iowa': 130, 'sports': 131, 'california': 132, 'public safety': 133, 'bush administration': 134, 'race and ethnicity': 135, 'guns': 136, \"politifact's top promises\": 137, 'pennsylvania': 138, 'sexuality': 139, 'abortion': 140, 'china': 141, 'disability': 142, 'agriculture': 143, 'energy': 144, 'social security': 145, 'marriage': 146, 'patriotism': 147, 'families': 148, 'economy': 149, 'homeland security': 150, 'climate change': 151, 'russia': 152, 'recreation': 153, 'public service': 154, 'financial regulation': 155, 'pundits': 156, 'trade': 157, 'afghanistan': 158, 'civil rights': 159, 'iran': 160, 'impeachment': 161, 'income': 162, 'history': 163, 'border security': 164, 'public health': 165, 'redistricting': 166, 'north carolina': 167, 'food': 168, 'national': 169, 'small business': 170, 'urban': 171, 'georgia': 172, 'vermont': 173, 'wisconsin': 174, 'constitutional amendments': 175, 'debates': 176, 'science': 177}\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def build_descriptive_text_vocab_subject_stateInfo(input_text):\n",
    "    vocab = set()\n",
    "    vocab.add(\"<UNK>\")\n",
    "    input_text = input_text.str.lower()\n",
    "    input_text = input_text.fillna(\"<NONE>\")\n",
    "    input_text = input_text.astype(str)\n",
    "\n",
    "    # Build vocabulary\n",
    "    for text in input_text:\n",
    "        for word in text.split(\";\"):\n",
    "            word = word.strip()  # Remove extra spaces\n",
    "            if word:\n",
    "                vocab.add(word)\n",
    "\n",
    "    return {token: i for i, token in enumerate(vocab)}\n",
    "\n",
    "def vectorize_descriptive_text_subject(input_text, vocab):\n",
    "  # Ensure the input is a string\n",
    "  if isinstance(input_text, list):\n",
    "      input_text = \";\".join(input_text)  # Join list into a string\n",
    "  vectorized_text = np.zeros(len(vocab))\n",
    "  for word in input_text.split(\";\"):\n",
    "    if word in vocab:\n",
    "      vectorized_text[vocab[word]] += 1\n",
    "    else:\n",
    "      vectorized_text[vocab[\"<UNK>\"]] += 1\n",
    "  return vectorized_text\n",
    "\n",
    "V_subject = build_descriptive_text_vocab_subject_stateInfo(df[\"subject\"])\n",
    "V_state = build_descriptive_text_vocab_subject_stateInfo(df[\"state_info\"])\n",
    "\n",
    "print(V_subject)\n",
    "test = [\"bankruptcy\", \"infrastructure\", \"well\", \"NakiyahDhariwala\"]\n",
    "check = vectorize_descriptive_text_subject(test, V_subject)\n",
    "print(check)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def build_descriptive_text_vocab(input_text):\\n  vocab = set()\\n  vocab.add(\"<UNK>\")\\n  for text in input_text:\\n    for word in text.split():\\n      word = remove_punctuation(word)\\n      if word:\\n        vocab.add(word)\\n  return {token: i for i, token in enumerate(vocab)}\\n\\ndef vectorize_descriptive_text(input_text, vocab):\\n  vectorized_text = np.zeros(len(vocab))\\n  for word in input_text.split():\\n    word = remove_punctuation(word)\\n    if word in vocab:\\n      vectorized_text[vocab[word]] += 1\\n    else:\\n      vectorized_text[vocab[\"<UNK>\"]] += 1\\n  return vectorized_text\\n\\ndef remove_punctuation(word):\\n    punctuation = set([\".\", \"(\", \")\", \",\", \";\", \"?\", \"!\", \\'\"\\', \":\", \"\\'\"])\\n    while word and word[0] in punctuation:\\n        word = word[1:]\\n    while word and word[-1] in punctuation:\\n        word = word[:-1]\\n    return word.lower()\\n\\ntrain_data = pd.read_csv(\\'LIAR2/train.csv\\')\\n# V = build_descriptive_text_vocab(train_data[\\'statement\\'])\\nV = build_descriptive_text_vocab\\n\\ntest_statement = \"percent chores chores stunt chores stunt chores ontario the\"\\ntest_vector = vectorize_descriptive_text(test_statement, V)\\nprint(test_vector)\\nfor i in test_vector:\\n  print(i)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\"\"ORIGINAL CODE\n",
    "\n",
    "def build_descriptive_text_vocab(input_text):\n",
    "  vocab = set()\n",
    "  vocab.add(\"<UNK>\")\n",
    "  for text in input_text:\n",
    "    for word in text.split():\n",
    "      word = remove_punctuation(word)\n",
    "      if word:\n",
    "        vocab.add(word)\n",
    "  return {token: i for i, token in enumerate(vocab)}\n",
    "\n",
    "def vectorize_descriptive_text(input_text, vocab):\n",
    "  vectorized_text = np.zeros(len(vocab))\n",
    "  for word in input_text.split():\n",
    "    word = remove_punctuation(word)\n",
    "    if word in vocab:\n",
    "      vectorized_text[vocab[word]] += 1\n",
    "    else:\n",
    "      vectorized_text[vocab[\"<UNK>\"]] += 1\n",
    "  return vectorized_text\n",
    "\n",
    "def remove_punctuation(word):\n",
    "    punctuation = set([\".\", \"(\", \")\", \",\", \";\", \"?\", \"!\", '\"', \":\", \"'\"])\n",
    "    while word and word[0] in punctuation:\n",
    "        word = word[1:]\n",
    "    while word and word[-1] in punctuation:\n",
    "        word = word[:-1]\n",
    "    return word.lower()\n",
    "\n",
    "train_data = pd.read_csv('LIAR2/train.csv')\n",
    "# V = build_descriptive_text_vocab(train_data['statement'])\n",
    "V = build_descriptive_text_vocab\n",
    "\n",
    "test_statement = \"percent chores chores stunt chores stunt chores ontario the\"\n",
    "test_vector = vectorize_descriptive_text(test_statement, V)\n",
    "print(test_vector)\n",
    "for i in test_vector:\n",
    "  print(i)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomCSVDataset(Dataset):\n",
    "#     def __init__(self, csv_file, transform=None):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             csv_file (str): Path to the CSV file containing the data.\n",
    "#             transform (callable, optional): Optional transform to be applied on a sample.\n",
    "#         \"\"\"\n",
    "#         self.data = pd.read_csv(csv_file)  # Load the CSV file\n",
    "#         self.transform = transform\n",
    "\n",
    "#         # Assuming the last column is the label\n",
    "#         self.features = self.data.iloc[:, :-1].values  # All columns except the last one (features)\n",
    "#         self.labels = self.data.iloc[:, -1].values    # The last column (label)\n",
    "\n",
    "#         # Standardize features if necessary\n",
    "#         self.scaler = StandardScaler()\n",
    "#         self.features = self.scaler.fit_transform(self.features)  # Normalize features\n",
    "\n",
    "#     def __len__(self):\n",
    "#         \"\"\"Returns the total number of samples.\"\"\"\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             idx (int): Index of the sample to fetch.\n",
    "#         Returns:\n",
    "#             tuple: (features, label) where features are the input tensor and label is the output tensor.\n",
    "#         \"\"\"\n",
    "#         features = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "#         label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "#         if self.transform:\n",
    "#             features = self.transform(features)\n",
    "\n",
    "#         return features, label\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
