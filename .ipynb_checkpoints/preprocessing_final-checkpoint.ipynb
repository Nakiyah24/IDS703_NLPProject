{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statement, speaker_description, justification\n",
    "\n",
    "\n",
    "def build_descriptive_text_vocab_ashley(input_text):\n",
    "    vocab = set()\n",
    "    vocab.add(\"<UNK>\")\n",
    "    for text in input_text:\n",
    "        for word in text.split():\n",
    "            word = remove_punctuation_ashley(word)\n",
    "            if word:\n",
    "                vocab.add(word)\n",
    "    return {token: i for i, token in enumerate(vocab)}\n",
    "\n",
    "\n",
    "def vectorize_descriptive_text_ashley(input_text, vocab):\n",
    "    vectorized_text = np.zeros(len(vocab))\n",
    "    for word in input_text.split():\n",
    "        word = remove_punctuation_ashley(word)\n",
    "        if word in vocab:\n",
    "            vectorized_text[vocab[word]] += 1\n",
    "        else:\n",
    "            vectorized_text[vocab[\"<UNK>\"]] += 1\n",
    "    return vectorized_text\n",
    "\n",
    "\n",
    "def remove_punctuation_ashley(word):\n",
    "    punctuation = set([\".\", \"(\", \")\", \",\", \";\", \"?\", \"!\", '\"', \":\", \"'\"])\n",
    "    while word and word[0] in punctuation:\n",
    "        word = word[1:]\n",
    "    while word and word[-1] in punctuation:\n",
    "        word = word[:-1]\n",
    "    return word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding of subjects and context\n",
    "\n",
    "\n",
    "# for the subject and context columns we only need to add each row into the vocab list and see if there are any repetitions\n",
    "def build_descriptive_text_vocab_nruta(input_text):\n",
    "    vocab = set()\n",
    "    input_text = input_text.str.lower()\n",
    "    for word in input_text:\n",
    "        vocab.add(word)\n",
    "    vocab.add(\"<UNK>\")\n",
    "    return {token: i for i, token in enumerate(vocab)}\n",
    "\n",
    "\n",
    "def vectorize_descriptive_text_nruta(input_text, vocab):\n",
    "    vectorized_text = np.zeros(len(vocab))\n",
    "    for word in input_text:\n",
    "        if word in vocab:\n",
    "            vectorized_text[vocab[word]] += 1\n",
    "        else:\n",
    "            vectorized_text[vocab[\"<UNK>\"]] += 1\n",
    "    return vectorized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_descriptive_text_vocab_subject_stateInfo_nakiyah(input_text):\n",
    "    vocab = set()\n",
    "    vocab.add(\"<UNK>\")\n",
    "    input_text = input_text.str.lower()\n",
    "    input_text = input_text.astype(str)\n",
    "\n",
    "    # Build vocabulary\n",
    "    for text in input_text:\n",
    "        for word in text.split(\";\"):\n",
    "            word = word.strip()  # Remove extra spaces\n",
    "            if word:\n",
    "                vocab.add(word)\n",
    "\n",
    "    return {token: i for i, token in enumerate(vocab)}\n",
    "\n",
    "\n",
    "def vectorize_descriptive_text_subject_nakiyah(input_text, vocab):\n",
    "    # Ensure the input is a string\n",
    "    if isinstance(input_text, list):\n",
    "        input_text = \";\".join(input_text)  # Join list into a string\n",
    "    vectorized_text = np.zeros(len(vocab))\n",
    "    for word in input_text.split(\";\"):\n",
    "        if word in vocab:\n",
    "            vectorized_text[vocab[word]] += 1\n",
    "        else:\n",
    "            vectorized_text[vocab[\"<UNK>\"]] += 1\n",
    "    return vectorized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting csv to a PyTorch Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transform\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.decomposition import SparsePCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(data, n_components=500):\n",
    "    print(\"starting pca\")\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    print(\"scaled data\")\n",
    "    \n",
    "    spca = SparsePCA(n_components=n_components)\n",
    "    pca_transformed = spca.fit_transform(scaled_data)\n",
    "    print(\"pca done\")\n",
    "    \n",
    "    return pca_transformed, spca\n",
    "\n",
    "def safe_concatenate(row):\n",
    "    arrays = [x if isinstance(x, np.ndarray) else np.array([x]) for x in row.values]\n",
    "    return np.concatenate(arrays)\n",
    "\n",
    "def process_data(df, vocabs):\n",
    "    \"\"\"\n",
    "    Processes a dataset file to encode categorical variables and convert data into PyTorch tensors.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "        - features_tensor (torch.Tensor) : Tensor of features\n",
    "        - labels_tensor (torch.Tensor) : Tensor of labels.\n",
    "        - label_encoders (dict): Dictionary of LabelEncoders for categorical columns\n",
    "    \"\"\"\n",
    "    print(\"processing data\")\n",
    "    # drop useless data\n",
    "    dropped_columns = [\"id\", \"date\"]\n",
    "    df = df.drop(dropped_columns, axis=1)\n",
    "    \n",
    "    for col in [\"statement\", \"justification\", \"speaker_description\"]:\n",
    "        df[col] = df[col].fillna(\"None\").astype(str)\n",
    "        df[col] = df[col].apply(lambda x: vectorize_descriptive_text_ashley(x, vocabs[col]))\n",
    "        \n",
    "    for col in [\"subject\", \"state_info\"]:\n",
    "        df[col] = df[col].fillna(\"None\").astype(str)\n",
    "        df[col] = df[col].apply(lambda x: vectorize_descriptive_text_subject_nakiyah(x, vocabs[col]))\n",
    "        \n",
    "    for col in [\"speaker\", \"context\"]:\n",
    "        df[col] = df[col].fillna(\"None\").astype(str)\n",
    "        df[col] = df[col].apply(lambda x: vectorize_descriptive_text_nruta(x, vocabs[col]))\n",
    "        \n",
    "#     feature_matrix = np.stack(df.apply(safe_concatenate, axis=1).to_numpy())\n",
    "#     print(feature_matrix.shape)\n",
    "#     print(\"done processing data\")\n",
    "    \n",
    "#     pca_features, _ = apply_pca(feature_matrix)\n",
    "    \n",
    "    # return pd.DataFrame(pca_features)\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_vocabs():\n",
    "    df = pd.read_csv(\"data/train.csv\")\n",
    "    dropped_columns = [\"id\", \"date\"]\n",
    "    df = df.drop(dropped_columns, axis=1)\n",
    "    \n",
    "    vocabs = {}\n",
    "    for col in [\"statement\", \"justification\", \"speaker_description\"]:\n",
    "        df[col] = df[col].fillna(\"None\").astype(str)\n",
    "        V = build_descriptive_text_vocab_ashley(df[col])\n",
    "        df[col] = df[col].apply(lambda x: vectorize_descriptive_text_ashley(x, V))\n",
    "        vocabs[col] = V\n",
    "        \n",
    "    for col in [\"subject\", \"state_info\"]:\n",
    "        df[col] = df[col].fillna(\"None\").astype(str)\n",
    "        V = build_descriptive_text_vocab_subject_stateInfo_nakiyah(df[col])\n",
    "        df[col] = df[col].apply(lambda x: vectorize_descriptive_text_subject_nakiyah(x, V))\n",
    "        vocabs[col] = V\n",
    "        \n",
    "    for col in [\"speaker\", \"context\"]:\n",
    "        df[col] = df[col].fillna(\"None\").astype(str)\n",
    "        V = build_descriptive_text_vocab_nruta(df[col])\n",
    "        df[col] = df[col].apply(lambda x: vectorize_descriptive_text_nruta(x, V))\n",
    "        vocabs[col] = V\n",
    "    return vocabs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, path, vocabs, transform=None):\n",
    "        self.sentiment = pd.read_csv(path)\n",
    "        self.sentiment = process_data(self.sentiment, vocabs)\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentiment)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.sentiment.iloc[idx]\n",
    "        label = data[\"label\"]\n",
    "        data = data.drop(\"label\")\n",
    "        \n",
    "        max_length = 0\n",
    "        for col in data.index:\n",
    "            value = data[col]\n",
    "            if isinstance(value, (np.ndarray, list)):\n",
    "                max_length = max(max_length, len(value))\n",
    "        \n",
    "        feature_vectors = []\n",
    "        for col in data.index:\n",
    "            value = data[col]\n",
    "            if isinstance(value, (np.ndarray, list)):\n",
    "                feature_vectors.append(np.array(value))\n",
    "            else:\n",
    "                feature_vectors.append(np.array([value], dtype=np.float32))\n",
    "\n",
    "        feature_vectors = np.concatenate(feature_vectors)\n",
    "\n",
    "        if self.transform:\n",
    "            feature_vectors = self.transform(feature_vectors)\n",
    "            \n",
    "        return torch.tensor(feature_vectors, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data\n",
      "torch.Size([1, 85446]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "t = transform.Compose([transform.ToTensor()])\n",
    "vocabs = create_vocabs()\n",
    "train_dataset = SentimentDataset(path=\"data/train.csv\", vocabs=vocabs)\n",
    "dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "iterator = iter(dataloader)\n",
    "data, label = next(iterator)\n",
    "\n",
    "print(data.shape, label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeNewsClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=74529, num_classes=6):\n",
    "        super(FakeNewsClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 500)\n",
    "        self.bn1 = nn.BatchNorm1d(500)\n",
    "        self.fc2 = nn.Linear(500, 20)\n",
    "        self.bn2 = nn.BatchNorm1d(20)\n",
    "        self.fc3 = nn.Linear(20, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "vocabs = create_vocabs()\n",
    "train_dataset = SentimentDataset(path=\"data/train.csv\", vocabs=vocabs)\n",
    "test_dataset = SentimentDataset(path=\"data/test.csv\", vocabs=vocabs)\n",
    "val_dataset = SentimentDataset(path=\"data/valid.csv\", vocabs=vocabs)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 144/144 [03:46<00:00,  1.57s/its]\n",
      "Validating: 100%|██████████| 18/18 [00:27<00:00,  1.55s/its]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Loss:  1.6627, Val Loss: 1.6089054942131042, Train Accuracy:  41.65, Val Accuracy:  48.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 144/144 [03:43<00:00,  1.55s/its]\n",
      "Validating: 100%|██████████| 18/18 [00:27<00:00,  1.52s/its]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: Train Loss:  1.5654, Val Loss: 1.583156989680396, Train Accuracy:  51.06, Val Accuracy:  47.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 121/144 [03:10<00:36,  1.58s/its]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_787/3174221608.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"its\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = FakeNewsClassifier().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=.001, weight_decay=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 10\n",
    "training_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    train_correct_predictions = 0\n",
    "    train_total_samples = 0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "    \n",
    "    model.train()\n",
    "    for features, labels in tqdm(train_loader, desc=\"Training\", unit=\"its\"):\n",
    "        features = features.to(device).float()\n",
    "        labels = labels.to(device).long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        t_loss = criterion(outputs, labels)\n",
    "        t_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += t_loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the class with the highest score\n",
    "        train_correct_predictions += (predicted == labels).sum().item()\n",
    "        train_total_samples += labels.size(0)\n",
    "        \n",
    "    model.eval()\n",
    "    for features, labels in tqdm(val_loader, desc=\"Validating\", unit=\"its\"):\n",
    "        features = features.to(device).float()\n",
    "        labels = labels.to(device).long()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(features)\n",
    "            v_loss = criterion(outputs, labels)\n",
    "        \n",
    "        val_loss += v_loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        val_correct_predictions += (predicted == labels).sum().item()\n",
    "        val_total_samples += labels.size(0)\n",
    "        \n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    training_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracy = train_correct_predictions / train_total_samples * 100\n",
    "    val_accuracy = val_correct_predictions / val_total_samples * 100\n",
    "    print(f\"Epoch {epoch}/{epochs}: Train Loss: {train_loss: .4f}, Val Loss: {val_loss}, Train Accuracy: {train_accuracy: .2f}, Val Accuracy: {val_accuracy: .2f}\")\n",
    "    \n",
    "torch.save(model.state_dict(), \"trained_model.pth\")\n",
    "\n",
    "plt.plot(epochs, training_losses, label=\"Train\")\n",
    "plt.plot(epochs, val_losses, label=\"Validation\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Evolution of loss during training\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = FakeNewsClassifier().to(device)\n",
    "test_model.load_state_dict(torch.load(\"trained_model.pth\"))\n",
    "test_model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, labels in tqdm(test_loader, desc=\"Testing\", unit=\"batch\"):\n",
    "        features = features.to(device).float()\n",
    "        labels = labels.to(device).long()\n",
    "        \n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate Metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions, average=\"weighted\")\n",
    "recall = recall_score(all_labels, all_predictions, average=\"weighted\")\n",
    "f1 = f1_score(all_labels, all_predictions, average=\"weighted\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_predictions))\n",
    "\n",
    "corresponding_labels = [\"Pants on Fire\", \"False\", \"Barely True\", \"Half True\", \"Mostly True\", \"True\"]\n",
    "ConfusionMatrixDisplay.from_predictions(all_labels, all_predictions, display_labels=corresponding_labels)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.save(\"results/confusion_matrix.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
