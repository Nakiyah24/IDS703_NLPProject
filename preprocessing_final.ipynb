{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statement, speaker_description, justification\n",
    "\n",
    "\n",
    "def build_descriptive_text_vocab_ashley(input_text):\n",
    "    vocab = set()\n",
    "    vocab.add(\"<UNK>\")\n",
    "    for text in input_text:\n",
    "        for word in text.split():\n",
    "            word = remove_punctuation_ashley(word)\n",
    "            if word:\n",
    "                vocab.add(word)\n",
    "    return {token: i for i, token in enumerate(vocab)}\n",
    "\n",
    "\n",
    "def vectorize_descriptive_text_ashley(input_text, vocab):\n",
    "    vectorized_text = np.zeros(len(vocab))\n",
    "    for word in input_text.split():\n",
    "        word = remove_punctuation_ashley(word)\n",
    "        if word in vocab:\n",
    "            vectorized_text[vocab[word]] += 1\n",
    "        else:\n",
    "            vectorized_text[vocab[\"<UNK>\"]] += 1\n",
    "    return vectorized_text\n",
    "\n",
    "\n",
    "def remove_punctuation_ashley(word):\n",
    "    punctuation = set([\".\", \"(\", \")\", \",\", \";\", \"?\", \"!\", '\"', \":\", \"'\"])\n",
    "    while word and word[0] in punctuation:\n",
    "        word = word[1:]\n",
    "    while word and word[-1] in punctuation:\n",
    "        word = word[:-1]\n",
    "    return word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding of subjects and context\n",
    "\n",
    "\n",
    "# for the subject and context columns we only need to add each row into the vocab list and see if there are any repetitions\n",
    "def build_descriptive_text_vocab_nruta(input_text):\n",
    "    vocab = set()\n",
    "    input_text = input_text.str.lower()\n",
    "    for word in input_text:\n",
    "        vocab.add(word)\n",
    "    vocab.add(\"<UNK>\")\n",
    "    return {token: i for i, token in enumerate(vocab)}\n",
    "\n",
    "\n",
    "def vectorize_descriptive_text_nruta(input_text, vocab):\n",
    "    vectorized_text = np.zeros(len(vocab))\n",
    "    for word in input_text:\n",
    "        if word in vocab:\n",
    "            vectorized_text[vocab[word]] += 1\n",
    "        else:\n",
    "            vectorized_text[vocab[\"<UNK>\"]] += 1\n",
    "    return vectorized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_descriptive_text_vocab_subject_stateInfo_nakiyah(input_text):\n",
    "    vocab = set()\n",
    "    vocab.add(\"<UNK>\")\n",
    "    input_text = input_text.str.lower()\n",
    "    input_text = input_text.astype(str)\n",
    "\n",
    "    # Build vocabulary\n",
    "    for text in input_text:\n",
    "        for word in text.split(\";\"):\n",
    "            word = word.strip()  # Remove extra spaces\n",
    "            if word:\n",
    "                vocab.add(word)\n",
    "\n",
    "    return {token: i for i, token in enumerate(vocab)}\n",
    "\n",
    "\n",
    "def vectorize_descriptive_text_subject_nakiyah(input_text, vocab):\n",
    "    # Ensure the input is a string\n",
    "    if isinstance(input_text, list):\n",
    "        input_text = \";\".join(input_text)  # Join list into a string\n",
    "    vectorized_text = np.zeros(len(vocab))\n",
    "    for word in input_text.split(\";\"):\n",
    "        if word in vocab:\n",
    "            vectorized_text[vocab[word]] += 1\n",
    "        else:\n",
    "            vectorized_text[vocab[\"<UNK>\"]] += 1\n",
    "    return vectorized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting this to a PyTorch file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df):\n",
    "    \"\"\"\n",
    "    Processes a dataset file to encode categorical variables and convert data into PyTorch tensors.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "        - features_tensor (torch.Tensor) : Tensor of features\n",
    "        - labels_tensor (torch.Tensor) : Tensor of labels.\n",
    "        - label_encoders (dict): Dictionary of LabelEncoders for categorical columns\n",
    "    \"\"\"\n",
    "    # drop useless data\n",
    "    dropped_columns = [\"id\", \"date\"]\n",
    "    df = df.drop(dropped_columns, axis=1)\n",
    "    \n",
    "    for col in [\"statement\", \"justification\", \"speaker_description\"]:\n",
    "        df[col] = df[col].fillna(\"None\").astype(str)\n",
    "        V = build_descriptive_text_vocab_ashley(df[col])\n",
    "        df[col] = df[col].apply(lambda x: vectorize_descriptive_text_ashley(x, V))\n",
    "        \n",
    "    for col in [\"subject\", \"state_info\"]:\n",
    "        df[col] = df[col].fillna(\"None\").astype(str)\n",
    "        V = build_descriptive_text_vocab_subject_stateInfo_nakiyah(df[col])\n",
    "        df[col] = df[col].apply(lambda x: vectorize_descriptive_text_subject_nakiyah(x, V))\n",
    "        \n",
    "    for col in [\"speaker\", \"context\"]:\n",
    "        df[col] = df[col].fillna(\"None\").astype(str)\n",
    "        V = build_descriptive_text_vocab_nruta(df[col])\n",
    "        df[col] = df[col].apply(lambda x: vectorize_descriptive_text_nruta(x, V))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.sentiment = pd.read_csv(path)\n",
    "        self.sentiment = process_data(self.sentiment)\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentiment)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.sentiment.iloc[idx]\n",
    "        label = data[\"label\"]\n",
    "        data = data.drop(\"label\")\n",
    "        \n",
    "        max_length = 0\n",
    "        for col in data.index:\n",
    "            value = data[col]\n",
    "            if isinstance(value, (np.ndarray, list)):\n",
    "                max_length = max(max_length, len(value))\n",
    "        \n",
    "        feature_matrix = []\n",
    "        for col in data.index:\n",
    "            value = data[col]\n",
    "            \n",
    "            # Handle numeric columns or vectorized text\n",
    "            if isinstance(value, (np.ndarray, list)):\n",
    "                if len(value) != max_length:\n",
    "                    value = np.concatenate([np.array(value), np.zeros(max_length - len(value))])\n",
    "            else:\n",
    "                value = np.concatenate([np.array([value]), np.zeros(max_length - 1)])\n",
    "            feature_matrix.append(np.array(value))\n",
    "        feature_matrix = np.array(feature_matrix).T\n",
    "        print(feature_matrix.shape)\n",
    "\n",
    "        if self.transform:\n",
    "            feature_matrix = self.transform(feature_matrix)\n",
    "            \n",
    "        return torch.tensor(feature_matrix, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45438, 13)\n",
      "(45438, 13)\n",
      "tensor([[[  0.,   0.,   0.,  ...,   1.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         ...,\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
      "\n",
      "        [[  0.,   0.,   0.,  ..., 570.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         ...,\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]]) tensor([4., 1.])\n"
     ]
    }
   ],
   "source": [
    "t = transform.Compose([transform.ToTensor()])\n",
    "train_dataset = SentimentDataset(path=\"data/train.csv\")\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "iterator = iter(dataloader)\n",
    "data, label = next(iterator)\n",
    "\n",
    "print(data, label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
