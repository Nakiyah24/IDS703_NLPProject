{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transform\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_one_hot_vocab(input_text):\n",
    "    vocab = set()\n",
    "    input_text = input_text.str.lower()\n",
    "    for word in input_text:\n",
    "        vocab.add(word)\n",
    "    vocab.add(\"<UNK>\")\n",
    "    return {token: i for i, token in enumerate(vocab)}\n",
    "\n",
    "\n",
    "def one_hot_encode(input_text, vocab):\n",
    "    vectorized_text = np.zeros(len(vocab))\n",
    "    for word in input_text:\n",
    "        if word in vocab:\n",
    "            vectorized_text[vocab[word]] += 1\n",
    "        else:\n",
    "            vectorized_text[vocab[\"<UNK>\"]] += 1\n",
    "    return vectorized_text\n",
    "\n",
    "def build_specialized_vocab(input_text):\n",
    "    vocab = set()\n",
    "    vocab.add(\"<UNK>\")\n",
    "    input_text = input_text.str.lower().astype(str)\n",
    "\n",
    "    # Build vocabulary\n",
    "    for text in input_text:\n",
    "        for word in text.split(\";\"):\n",
    "            word = word.strip()  # Remove extra spaces\n",
    "            if word:\n",
    "                vocab.add(word)\n",
    "\n",
    "    return {token: i for i, token in enumerate(vocab)}\n",
    "\n",
    "\n",
    "def vectorize_text(input_text, vocab):\n",
    "    # Ensure the input is a string\n",
    "    vectorized_text = np.zeros(len(vocab))\n",
    "    for word in input_text.split(\";\"):\n",
    "        if word in vocab:\n",
    "            vectorized_text[vocab[word]] += 1\n",
    "        else:\n",
    "            vectorized_text[vocab[\"<UNK>\"]] += 1\n",
    "    return vectorized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting csv to a PyTorch Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df, vocabs):\n",
    "    \"\"\"\n",
    "    Processes a dataset file to encode categorical variables and convert data into PyTorch tensors.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "        - features_tensor (torch.Tensor) : Tensor of features\n",
    "        - labels_tensor (torch.Tensor) : Tensor of labels.\n",
    "        - label_encoders (dict): Dictionary of LabelEncoders for categorical columns\n",
    "    \"\"\"\n",
    "    print(\"processing data\")\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # drop useless data\n",
    "    dropped_columns = [\"id\", \"date\"]\n",
    "    df = df.drop(dropped_columns, axis=1)\n",
    "    \n",
    "    for col in [\"statement\", \"justification\", \"speaker_description\"]:\n",
    "        df[col] = df[col].fillna(\"None\").astype(str)\n",
    "        df[col] = df[col].apply(lambda x: tokenizer.encode(x, padding='max_length', truncation=True, max_length=256))\n",
    "        \n",
    "    for col in [\"subject\", \"state_info\"]:\n",
    "        df[col] = df[col].fillna(\"None\").astype(str)\n",
    "        df[col] = df[col].apply(lambda x: vectorize_text(x, vocabs[col]))\n",
    "        \n",
    "    for col in [\"speaker\", \"context\"]:\n",
    "        df[col] = df[col].fillna(\"None\").astype(str)\n",
    "        df[col] = df[col].apply(lambda x: one_hot_encode(x, vocabs[col]))\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_vocabs():\n",
    "    df = pd.read_csv(\"data/train.csv\")\n",
    "    dropped_columns = [\"id\", \"date\"]\n",
    "    df = df.drop(dropped_columns, axis=1)\n",
    "    \n",
    "    vocabs = {}\n",
    "        \n",
    "    for col in [\"subject\", \"state_info\"]:\n",
    "        df[col] = df[col].fillna(\"None\").astype(str)\n",
    "        vocabs[col] = build_specialized_vocab(df[col])\n",
    "        \n",
    "    for col in [\"speaker\", \"context\"]:\n",
    "        df[col] = df[col].fillna(\"None\").astype(str)\n",
    "        vocabs[col] = build_one_hot_vocab(df[col])\n",
    "    return vocabs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, path, vocabs, transform=None):\n",
    "        self.sentiment = pd.read_csv(path)\n",
    "        self.sentiment = process_data(self.sentiment, vocabs)\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentiment)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.sentiment.iloc[idx]\n",
    "        label = data[\"label\"]\n",
    "        data = data.drop(\"label\")\n",
    "        \n",
    "        max_length = 0\n",
    "        for col in data.index:\n",
    "            value = data[col]\n",
    "            if isinstance(value, (np.ndarray, list)):\n",
    "                max_length = max(max_length, len(value))\n",
    "        \n",
    "        feature_vectors = []\n",
    "        for col in data.index:\n",
    "            value = data[col]\n",
    "            if isinstance(value, (np.ndarray, list)):\n",
    "                feature_vectors.append(np.array(value))\n",
    "            else:\n",
    "                feature_vectors.append(np.array([value], dtype=np.float32))\n",
    "\n",
    "        feature_vectors = np.concatenate(feature_vectors)\n",
    "\n",
    "        if self.transform:\n",
    "            feature_vectors = self.transform(feature_vectors)\n",
    "            \n",
    "        return torch.tensor(feature_vectors, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d85bf752b244c6a88bb34a57736d3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb7f6e704f340868b9cfb477e1a306a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53a62db7a494d1fa48cb417ea3c71fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba07d7d544294ce48c4bdf5f2be41ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9638]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "t = transform.Compose([transform.ToTensor()])\n",
    "vocabs = create_vocabs()\n",
    "train_dataset = SentimentDataset(path=\"data/train.csv\", vocabs=vocabs)\n",
    "dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "iterator = iter(dataloader)\n",
    "data, label = next(iterator)\n",
    "\n",
    "print(data.shape, label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeNewsClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=9638, num_classes=6):\n",
    "        super(FakeNewsClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.fc4 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data\n",
      "processing data\n",
      "processing data\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "vocabs = create_vocabs()\n",
    "train_dataset = SentimentDataset(path=\"data/train.csv\", vocabs=vocabs)\n",
    "test_dataset = SentimentDataset(path=\"data/test.csv\", vocabs=vocabs)\n",
    "val_dataset = SentimentDataset(path=\"data/valid.csv\", vocabs=vocabs)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 2.69 GiB total capacity; 0 bytes already allocated; 0 bytes free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2894/2219415060.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFakeNewsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 2.69 GiB total capacity; 0 bytes already allocated; 0 bytes free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model = FakeNewsClassifier().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=.001, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 10\n",
    "training_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    train_correct_predictions = 0\n",
    "    train_total_samples = 0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "    \n",
    "    model.train()\n",
    "    for features, labels in tqdm(train_loader, desc=\"Training\", unit=\"its\"):\n",
    "        features = features.to(device).float()\n",
    "        labels = labels.to(device).long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        t_loss = criterion(outputs, labels)\n",
    "        t_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += t_loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the class with the highest score\n",
    "        train_correct_predictions += (predicted == labels).sum().item()\n",
    "        train_total_samples += labels.size(0)\n",
    "        \n",
    "    model.eval()\n",
    "    for features, labels in tqdm(val_loader, desc=\"Validating\", unit=\"its\"):\n",
    "        features = features.to(device).float()\n",
    "        labels = labels.to(device).long()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(features)\n",
    "            v_loss = criterion(outputs, labels)\n",
    "        \n",
    "        val_loss += v_loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        val_correct_predictions += (predicted == labels).sum().item()\n",
    "        val_total_samples += labels.size(0)\n",
    "        \n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    training_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracy = train_correct_predictions / train_total_samples * 100\n",
    "    val_accuracy = val_correct_predictions / val_total_samples * 100\n",
    "    print(f\"Epoch {epoch}/{epochs}: Train Loss: {train_loss: .4f}, Val Loss: {val_loss}, Train Accuracy: {train_accuracy: .2f}, Val Accuracy: {val_accuracy: .2f}\")\n",
    "    \n",
    "torch.save(model.state_dict(), \"trained_model.pth\")\n",
    "\n",
    "plt.plot(np.arange(epochs), training_losses, label=\"Train\")\n",
    "plt.plot(np.arange(epochs), val_losses, label=\"Validation\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Evolution of loss during training\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = FakeNewsClassifier().to(device)\n",
    "test_model.load_state_dict(torch.load(\"trained_model.pth\"))\n",
    "test_model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, labels in tqdm(test_loader, desc=\"Testing\", unit=\"batch\"):\n",
    "        features = features.to(device).float()\n",
    "        labels = labels.to(device).long()\n",
    "        \n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate Metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions, average=\"weighted\")\n",
    "recall = recall_score(all_labels, all_predictions, average=\"weighted\")\n",
    "f1 = f1_score(all_labels, all_predictions, average=\"weighted\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_predictions))\n",
    "\n",
    "corresponding_labels = [\"Pants on Fire\", \"False\", \"Barely True\", \"Half True\", \"Mostly True\", \"True\"]\n",
    "ConfusionMatrixDisplay.from_predictions(all_labels, all_predictions, display_labels=corresponding_labels)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.save(\"results/test_confusion_matrix.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
