{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statement, speaker_description, justification\n",
    "\n",
    "\n",
    "def build_descriptive_text_vocab_ashley(input_text):\n",
    "    vocab = set()\n",
    "    vocab.add(\"<UNK>\")\n",
    "    for text in input_text:\n",
    "        for word in text.split():\n",
    "            word = remove_punctuation_ashley(word)\n",
    "            if word:\n",
    "                vocab.add(word)\n",
    "    return {token: i for i, token in enumerate(vocab)}\n",
    "\n",
    "\n",
    "def vectorize_descriptive_text_ashley(input_text, vocab):\n",
    "    vectorized_text = np.zeros(len(vocab))\n",
    "    for word in input_text.split():\n",
    "        word = remove_punctuation_ashley(word)\n",
    "        if word in vocab:\n",
    "            vectorized_text[vocab[word]] += 1\n",
    "        else:\n",
    "            vectorized_text[vocab[\"<UNK>\"]] += 1\n",
    "    return vectorized_text\n",
    "\n",
    "\n",
    "def remove_punctuation_ashley(word):\n",
    "    punctuation = set([\".\", \"(\", \")\", \",\", \";\", \"?\", \"!\", '\"', \":\", \"'\"])\n",
    "    while word and word[0] in punctuation:\n",
    "        word = word[1:]\n",
    "    while word and word[-1] in punctuation:\n",
    "        word = word[:-1]\n",
    "    return word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding of subjects and context\n",
    "\n",
    "\n",
    "# for the subject and context columns we only need to add each row into the vocab list and see if there are any repetitions\n",
    "def build_descriptive_text_vocab_nruta(input_text):\n",
    "    vocab = set()\n",
    "    input_text = input_text.str.lower()\n",
    "    for word in input_text:\n",
    "        vocab.add(word)\n",
    "    vocab.add(\"<UNK>\")\n",
    "    return {token: i for i, token in enumerate(vocab)}\n",
    "\n",
    "\n",
    "def vectorize_descriptive_text_nruta(input_text, vocab):\n",
    "    vectorized_text = np.zeros(len(vocab))\n",
    "    for word in input_text:\n",
    "        if word in vocab:\n",
    "            vectorized_text[vocab[word]] += 1\n",
    "        else:\n",
    "            vectorized_text[vocab[\"<UNK>\"]] += 1\n",
    "    return vectorized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_descriptive_text_vocab_subject_stateInfo_nakiyah(input_text):\n",
    "    vocab = set()\n",
    "    vocab.add(\"<UNK>\")\n",
    "    input_text = input_text.str.lower()\n",
    "    input_text = input_text.astype(str)\n",
    "\n",
    "    # Build vocabulary\n",
    "    for text in input_text:\n",
    "        for word in text.split(\";\"):\n",
    "            word = word.strip()  # Remove extra spaces\n",
    "            if word:\n",
    "                vocab.add(word)\n",
    "\n",
    "    return {token: i for i, token in enumerate(vocab)}\n",
    "\n",
    "\n",
    "def vectorize_descriptive_text_subject_nakiyah(input_text, vocab):\n",
    "    # Ensure the input is a string\n",
    "    if isinstance(input_text, list):\n",
    "        input_text = \";\".join(input_text)  # Join list into a string\n",
    "    vectorized_text = np.zeros(len(vocab))\n",
    "    for word in input_text.split(\";\"):\n",
    "        if word in vocab:\n",
    "            vectorized_text[vocab[word]] += 1\n",
    "        else:\n",
    "            vectorized_text[vocab[\"<UNK>\"]] += 1\n",
    "    return vectorized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting csv to a PyTorch Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transform\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.decomposition import SparsePCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(data, n_components=500):\n",
    "    print(\"starting pca\")\n",
    "    scaler = StandardScaler()\n",
    "    \"\"\"Standardizes the features so that they have a mean of 0 and a standard deviation of 1.\"\"\"\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    print(\"scaled data\")\n",
    "    \n",
    "    spca = SparsePCA(n_components=n_components) \n",
    "    \"\"\"\n",
    "    Applying Sparse PCA: Reduces the data dimensionality to n_components (default is 500). \n",
    "    Sparse PCA is used to enforce sparsity in the principal components, which is useful for large datasets.\n",
    "    \"\"\"\n",
    "    pca_transformed = spca.fit_transform(scaled_data)\n",
    "    print(\"pca done\")\n",
    "    \n",
    "    return pca_transformed, spca\n",
    "\n",
    "def safe_concatenate(row):\n",
    "    arrays = [x if isinstance(x, np.ndarray) else np.array([x]) for x in row.values]\n",
    "    return np.concatenate(arrays)\n",
    "\n",
    "def process_data(df, vocabs):\n",
    "    \"\"\"\n",
    "    Processes a dataset file to encode categorical variables and convert data into PyTorch tensors.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "        - features_tensor (torch.Tensor) : Tensor of features\n",
    "        - labels_tensor (torch.Tensor) : Tensor of labels.\n",
    "        - label_encoders (dict): Dictionary of LabelEncoders for categorical columns\n",
    "    \"\"\"\n",
    "    print(\"processing data\")\n",
    "    # drop useless data\n",
    "    dropped_columns = [\"id\", \"date\"]\n",
    "    df = df.drop(dropped_columns, axis=1)\n",
    "    \n",
    "    for col in [\"statement\", \"justification\", \"speaker_description\"]:\n",
    "        df[col] = df[col].fillna(\"None\").astype(str)\n",
    "        df[col] = df[col].apply(lambda x: vectorize_descriptive_text_ashley(x, vocabs[col]))\n",
    "        \n",
    "    for col in [\"subject\", \"state_info\"]:\n",
    "        df[col] = df[col].fillna(\"None\").astype(str)\n",
    "        df[col] = df[col].apply(lambda x: vectorize_descriptive_text_subject_nakiyah(x, vocabs[col]))\n",
    "        \n",
    "    for col in [\"speaker\", \"context\"]:\n",
    "        df[col] = df[col].fillna(\"None\").astype(str)\n",
    "        df[col] = df[col].apply(lambda x: vectorize_descriptive_text_nruta(x, vocabs[col]))\n",
    "        \n",
    "#     feature_matrix = np.stack(df.apply(safe_concatenate, axis=1).to_numpy())\n",
    "#     print(feature_matrix.shape)\n",
    "#     print(\"done processing data\")\n",
    "    \n",
    "#     pca_features, _ = apply_pca(feature_matrix)\n",
    "    \n",
    "    # return pd.DataFrame(pca_features)\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_vocabs():\n",
    "    df = pd.read_csv(\"data/train.csv\")\n",
    "    dropped_columns = [\"id\", \"date\"]\n",
    "    df = df.drop(dropped_columns, axis=1)\n",
    "    \n",
    "    vocabs = {}\n",
    "    for col in [\"statement\", \"justification\", \"speaker_description\"]:\n",
    "        df[col] = df[col].fillna(\"None\").astype(str)\n",
    "        V = build_descriptive_text_vocab_ashley(df[col])\n",
    "        df[col] = df[col].apply(lambda x: vectorize_descriptive_text_ashley(x, V))\n",
    "        vocabs[col] = V\n",
    "        \n",
    "    for col in [\"subject\", \"state_info\"]:\n",
    "        df[col] = df[col].fillna(\"None\").astype(str)\n",
    "        V = build_descriptive_text_vocab_subject_stateInfo_nakiyah(df[col])\n",
    "        df[col] = df[col].apply(lambda x: vectorize_descriptive_text_subject_nakiyah(x, V))\n",
    "        vocabs[col] = V\n",
    "        \n",
    "    for col in [\"speaker\", \"context\"]:\n",
    "        df[col] = df[col].fillna(\"None\").astype(str)\n",
    "        V = build_descriptive_text_vocab_nruta(df[col])\n",
    "        df[col] = df[col].apply(lambda x: vectorize_descriptive_text_nruta(x, V))\n",
    "        vocabs[col] = V\n",
    "    return vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data\n"
     ]
    }
   ],
   "source": [
    "file_path = pd.read_csv(\"data/train.csv\")\n",
    "V = create_vocabs()\n",
    "data = process_data(file_path, V)  #self.sentiment\n",
    "len_data = len(data) \n",
    "transform = None # self.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import pandas as pd\n",
    "data = {'Column1': [10, 20, 30], 'Column2': [40, 50, 60]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Iterate over DataFrame rows\n",
    "for idx, row in df.iterrows():\n",
    "    dfdata = df.iloc[idx]\n",
    "    \n",
    "    print(type(dfdata))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, row in data.iterrows():\n",
    "#     data_idx = data.iloc[idx]\n",
    "#     label = data_idx[\"label\"]\n",
    "#     data_idx = data_idx.drop(\"label\")\n",
    "#     # print(data_idx)\n",
    "#     # NOTE: Ashley data == data_idx\n",
    "    \n",
    "#     max_length = 0\n",
    "#     for col in data_idx.index: #index is going to be the colname\n",
    "#         # print(col)\n",
    "#         value = data_idx[col]\n",
    "#         # print(value)\n",
    "#     # print(data_idx)\n",
    "#     # Each row out here becomes a series\n",
    "    \n",
    "#     feature_vectors = []\n",
    "#     for col in data_idx.index: #index is going to be the colname\n",
    "#         # print(col)\n",
    "#         value = data_idx[col]\n",
    "        \n",
    "#         if isinstance(value, (np.ndarray, list)): \n",
    "#             feature_vectors.append(np.array(value))        \n",
    "#         else:\n",
    "#             feature_vectors.append(np.array([value], dtype=np.float32))\n",
    "#         \"\"\"\n",
    "#         If the value is a list or an array\n",
    "#         convert to numpy array and append to list\n",
    "#         \"\"\"\n",
    "#         feature_vectors = np.concatenate(feature_vectors)\n",
    "\n",
    "# print(feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nConverts data (e.g., lists, NumPy arrays) into a PyTorch tensor explicitly.\\nUsed when the data is already prepared in a numpy array or a list\\n\\nThis step ensures that feature_vectors (a NumPy array) and label are \\nconverted into PyTorch tensors before being returned by __getitem__.\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, path, vocabs, transform=None):\n",
    "        self.sentiment = pd.read_csv(path)\n",
    "        self.sentiment = process_data(self.sentiment, vocabs)\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentiment)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.sentiment.iloc[idx] \n",
    "        label = data[\"label\"]\n",
    "        data = data.drop(\"label\")\n",
    "        \n",
    "        \"\"\"data == each row of the df which becomes a series\n",
    "            each series item then gets converted to a\n",
    "            corresponding index and value\"\"\"\n",
    "        \n",
    "        max_length = 0\n",
    "        for col in data.index:\n",
    "            value = data[col]\n",
    "            if isinstance(value, (np.ndarray, list)):\n",
    "                max_length = max(max_length, len(value))\n",
    "                \n",
    "        \n",
    "        \n",
    "        feature_vectors = [] \n",
    "        \"\"\"The output feature_vectors is a list of NumPy arrays.\"\"\"\n",
    "\n",
    "        for col in data.index:\n",
    "            value = data[col]\n",
    "            if isinstance(value, (np.ndarray, list)):\n",
    "                feature_vectors.append(np.array(value))\n",
    "            else:\n",
    "                feature_vectors.append(np.array([value], dtype=np.float32))\n",
    "\n",
    "        feature_vectors = np.concatenate(feature_vectors)\n",
    "\n",
    "        if self.transform:\n",
    "            feature_vectors = self.transform(feature_vectors)\n",
    "            \n",
    "        \"\"\"If the input is an image (e.g., NumPy array or PIL image), \n",
    "        ToTensor scales pixel values from [0, 255] to [0, 1] for normalized training.\n",
    "        It's part of a transform.Compose pipeline that applies transformations in sequence.\n",
    "        \n",
    "        Here, ToTensor standardizes the input (if it were an image or raw data) \n",
    "        before converting it into a PyTorch tensor.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        return torch.tensor(feature_vectors, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "\"\"\"\n",
    "Converts data (e.g., lists, NumPy arrays) into a PyTorch tensor explicitly.\n",
    "Used when the data is already prepared in a numpy array or a list\n",
    "\n",
    "This step ensures that feature_vectors (a NumPy array) and label are \n",
    "converted into PyTorch tensors before being returned by __getitem__.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'Compose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompose\u001b[49m([transform\u001b[38;5;241m.\u001b[39mToTensor()])\n\u001b[1;32m      2\u001b[0m vocabs \u001b[38;5;241m=\u001b[39m create_vocabs()\n\u001b[1;32m      3\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m SentimentDataset(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, vocabs\u001b[38;5;241m=\u001b[39mvocabs, transform\u001b[38;5;241m=\u001b[39mt)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'Compose'"
     ]
    }
   ],
   "source": [
    "t = transform.Compose([transform.ToTensor()])\n",
    "vocabs = create_vocabs()\n",
    "train_dataset = SentimentDataset(path=\"data/train.csv\", vocabs=vocabs, transform=t)\n",
    "dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "iterator = iter(dataloader)\n",
    "data, label = next(iterator)\n",
    "\n",
    "print(data.shape, label.shape)\n",
    "\n",
    "\"\"\"\n",
    "The DataLoader simplifies the process of:\n",
    "\n",
    "1. Splitting a dataset into manageable batches.\n",
    "2. Shuffling the dataset for randomness, ensuring the model doesn't overfit to a fixed order of data.\n",
    "3. Loading data in parallel using multiple CPU cores for faster processing.\n",
    "4. Ensuring compatibility with PyTorch's training loop structure.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeNewsClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=74529, num_classes=6):\n",
    "        super(FakeNewsClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 500)\n",
    "        self.bn1 = nn.BatchNorm1d(500)\n",
    "        self.fc2 = nn.Linear(500, 20)\n",
    "        self.bn2 = nn.BatchNorm1d(20)\n",
    "        self.fc3 = nn.Linear(20, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "vocabs = create_vocabs()\n",
    "train_dataset = SentimentDataset(path=\"data/train.csv\", vocabs=vocabs)\n",
    "test_dataset = SentimentDataset(path=\"data/test.csv\", vocabs=vocabs)\n",
    "val_dataset = SentimentDataset(path=\"data/valid.csv\", vocabs=vocabs)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FakeNewsClassifier().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=.001, weight_decay=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 10\n",
    "training_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    train_correct_predictions = 0\n",
    "    train_total_samples = 0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "    \n",
    "    model.train()\n",
    "    for features, labels in tqdm(train_loader, desc=\"Training\", unit=\"its\"):\n",
    "        features = features.to(device).float()\n",
    "        labels = labels.to(device).long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        t_loss = criterion(outputs, labels)\n",
    "        t_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += t_loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the class with the highest score\n",
    "        train_correct_predictions += (predicted == labels).sum().item()\n",
    "        train_total_samples += labels.size(0)\n",
    "        \n",
    "    model.eval()\n",
    "    for features, labels in tqdm(val_loader, desc=\"Validating\", unit=\"its\"):\n",
    "        features = features.to(device).float()\n",
    "        labels = labels.to(device).long()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(features)\n",
    "            v_loss = criterion(outputs, labels)\n",
    "        \n",
    "        val_loss += v_loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        val_correct_predictions += (predicted == labels).sum().item()\n",
    "        val_total_samples += labels.size(0)\n",
    "        \n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    training_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracy = train_correct_predictions / train_total_samples * 100\n",
    "    val_accuracy = val_correct_predictions / val_total_samples * 100\n",
    "    print(f\"Epoch {epoch}/{epochs}: Train Loss: {train_loss: .4f}, Val Loss: {val_loss}, Train Accuracy: {train_accuracy: .2f}, Val Accuracy: {val_accuracy: .2f}\")\n",
    "    \n",
    "torch.save(model.state_dict(), \"trained_model.pth\")\n",
    "\n",
    "plt.plot(epochs, training_losses, label=\"Train\")\n",
    "plt.plot(epochs, val_losses, label=\"Validation\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Evolution of loss during training\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = FakeNewsClassifier().to(device)\n",
    "test_model.load_state_dict(torch.load(\"trained_model.pth\"))\n",
    "test_model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, labels in tqdm(test_loader, desc=\"Testing\", unit=\"batch\"):\n",
    "        features = features.to(device).float()\n",
    "        labels = labels.to(device).long()\n",
    "        \n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate Metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions, average=\"weighted\")\n",
    "recall = recall_score(all_labels, all_predictions, average=\"weighted\")\n",
    "f1 = f1_score(all_labels, all_predictions, average=\"weighted\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_predictions))\n",
    "\n",
    "corresponding_labels = [\"Pants on Fire\", \"False\", \"Barely True\", \"Half True\", \"Mostly True\", \"True\"]\n",
    "ConfusionMatrixDisplay.from_predictions(all_labels, all_predictions, display_labels=corresponding_labels)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.save(\"results/confusion_matrix.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
